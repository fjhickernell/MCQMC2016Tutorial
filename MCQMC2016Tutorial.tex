%Manuscript based on tutorial given by Fred J. Hickernell at MCQMC, August 14, 2016
%Requires graphics files
%  

\documentclass[graybox,footinfo]{svmult}

\smartqed
\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
% not available on your system
\usepackage{graphicx}       % standard LaTeX graphics tool
% when including figure files

\usepackage{array,colortbl}
\usepackage{amsmath,amsfonts,amssymb,bm} % no amsthm, Springer defines Theorem, 
%Lemma, etc themselves
%\usepackage[mathx]{mathabx}
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<5> <6> <7> <8> <9> <10>
	<10.95> <12> <14.4> <17.28> <20.74> <24.88>
	mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}      {0}{mathx}{"71}



% Note that Springer defines the following already:
%
% \D upright d for differential d
% \I upright i for imaginary unit
% \E upright e for exponential function
% \tens depicts tensors as sans serif upright
% \vec depicts vectors as boldface characters instead of the arrow accent
%
% Additionally we throw in the following common used macro's:
\input{macros}

% Macros below are now inclded in macros.tex from MCQMC 2016 web site
% This spot formerly included macros that are now in macros.tex

% indicator boldface 1:
\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}
%\newcommand{\ind}{\mathbbold{1}}


\usepackage{microtype} % good font tricks

\usepackage[colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black]{hyperref}
\urlstyle{same}
\usepackage{bookmark}
\pdfstringdefDisableCommands{\def\and{, }}
\makeatletter % to avoid hyperref warnings:
\providecommand*{\toclevel@author}{999}
\providecommand*{\toclevel@title}{0}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Fred's additions
%
\usepackage{xspace}
\DeclareMathOperator{\Cov}{cov}
\DeclareMathOperator{\algn}{CNF}
\DeclareMathOperator{\disc}{DSC}
\DeclareMathOperator{\Var}{VAR}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\GP}{\cg\!\!\cp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\newcommand{\FJHLessonZero}{The trio identity decomposes the cubature error as a 
product of three terms: the variation of the integrand, the discrepancy of the sampling 
measure, and the confounding. This identity shows how the integrand and the sampling 
measure each contribute to the cubature error.  }	

\newcommand{\FJHLessonOne}{The quality of a sampling scheme is important, not 
	merely the sample size.  Systematic sampling, even simple random sampling, may yield 
	a 
	much smaller error than haphazard sampling. }	

\newcommand{\FJHLessonTwo}{Quasi-Monte Carlo methods replace IID data sites by 
those that are more evenly spread than IID points, such as
Sobol' sequences and integration lattice nodeset sequences.  These sequences of evenly 
spread sampling measures yield discrepancies and cubature errors that decay to zero at 
a faster rate than those of IID sequences. }	

\newcommand{\FJHLessonThree}{Randomizing your sampling measure, not only may 
eliminate bias, but it may help you improve accuracy by avoiding the awful minority of 
possible sampling measures. }

\newcommand{\FJHLessonFour}{Well-chosen variable transformations may reduce 
cubature error by producing an integrand with a smaller variation than the original. }

\newcommand{\FJHLessonFive}{The benefits of sampling measures with lower 
discrepancy are limited to those problems where the integrand is well-behaved enough to 
have 
finite variation.}

\newcommand{\FJHLessonSix}{??. }
\newlength{\FJHfigheight}
\setlength{\FJHfigheight}{4 cm}


	
	\newtheorem{FJHLesson}{Lesson}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\std}{\textup{std}}
\newcommand{\Dt}{\textup{D}}
\newcommand{\Rn}{\textup{R}}
\newcommand{\Ba}{\textup{B}}


\newcommand{\MLE}{\text{MLE}}
\newcommand{\mC}{\mathsf{C}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\cf}{\mathcal{F}}
\newcommand{\cl}{\mathcal{L}}
\newcommand{\cn}{\mathcal{N}}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\cm}{\mathcal{M}}
\newcommand{\cg}{\mathcal{G}}
\newcommand{\cp}{\mathcal{P}}
\newcommand{\cu}{\mathcal{U}}
\newcommand{\cx}{\mathcal{X}}
\newcommand{\cy}{\mathcal{Y}}
\newcommand{\cz}{\mathcal{Z}}
\newcommand{\natm}{\N_{0,m}}
\newcommand{\cube}{[0,1)^d}
\newcommand{\hf}{\hat{f}}
\newcommand{\rf}{\mathring{f}}
\newcommand{\tf}{\tilde{f}}
\newcommand{\hg}{\hat{g}}
\newcommand{\hmu}{\widehat{\mu}}
\newcommand{\hI}{\hat{I}}
\newcommand{\tvk}{\tilde{\bsk}}
\newcommand{\hS}{\widehat{S}}
\newcommand{\tS}{\widetilde{S}}
\newcommand{\wcS}{\widecheck{S}}
\newcommand{\rnu}{\mathring{\nu}}
\newcommand{\tnu}{\widetilde{\nu}}
\newcommand{\hnu}{\widehat{\nu}}
\newcommand{\homega}{\widehat{\omega}}
\newcommand{\wcomega}{\mathring{\omega}}
\newcommand{\fC}{\mathfrak{C}}
\newcommand{\nodes}{\{\bsz_i\}_{i=0}^{\infty}}
\newcommand{\nodesn}{\{\bsz_i\}_{i=0}^{n-1}}
\newcommand{\norm}[1]{\ensuremath{\left \lVert #1 \right \rVert}} 
\newcommand{\snorm}[1]{\ensuremath{\left \lVert #1 \right \rVert}} 
\newcommand{\bignorm}[1]{\ensuremath{\bigl \lVert #1 \bigr \rVert}}
\newcommand{\Bignorm}[1]{\ensuremath{\Bigl \lVert #1 \Bigr \rVert}}
\newcommand{\abs}[1]{\ensuremath{\bigl \lvert #1 \bigr \rvert}}
\newcommand{\bigabs}[1]{\ensuremath{\bigl \lvert #1 \bigr \rvert}}
\newcommand{\Bigabs}[1]{\ensuremath{\Bigl \lvert #1 \Bigr \rvert}}
\newcommand{\biggabs}[1]{\ensuremath{\biggl \lvert #1 \biggr \rvert}}
\newcommand{\Biggabs}[1]{\ensuremath{\Biggl \lvert #1 \Biggr \rvert}}
\newcommand{\ip}[3][{}]{\ensuremath{\left \langle #2, #3 \right \rangle_{#1}}}
\newcommand{\err}{\textrm{err}}
\newcommand{\tbsone}{\tilde{\bsone}}
\newcommand{\tvarrho}{\tilde{\varrho}}
\newcommand{\rvarrho}{\mathring{\varrho}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tbsc}{\tilde{\bsc}}
\newcommand{\tbsy}{\tilde{\bsy}}
\newcommand{\tmC}{\widetilde{\mC}}
\newcommand{\tz}{\tilde{z}}
\newcommand{\tbsz}{\tilde{\bsz}}

\allowdisplaybreaks


\title*{Error Analysis for Quasi-Monte Carlo Methods}
\author{Fred J. Hickernell}
\institute{Fred J. Hickernell
\at Department of Applied Mathematics,  Illinois Institute of Technology, 10 W. 32$^{\text{nd}}$ Street, RE 208, Chicago, IL 60616, USA 
\email{hickernell@iit.edu}}
\maketitle

\abstract{.} 

\section{Introduction}
Monte Carlo methods are used to approximate multivariate integrals that cannot be evaluated analytically, i.e.,  integrals of the form
\begin{equation}
\mu = \int_{\cx} f(\bsx) \, \nu(\D \bsx), \label{INT} \tag{INT}
\end{equation}
where $f:\cx \to \R$ is a measurable function, $\cx$ is a measurable set, and $\nu$ is a 
\emph{probability} measure.  Here, $\mu$ is the mean of $f(\bsX)$, where the random 
variable $\bsX$ has probability measure $\nu$.  Monte Carlo methods approximate 
$\mu$ by a weighted average of values of $f$ at a finite number of data sites (points, 
nodes), $\bsx_1, \ldots, \bsx_n$:
\begin{equation} \label{AppxINT} \tag{MC}
\hmu = \sum_{i=1}^n f(\bsx_i) w_i = \int_{\cx} f(\bsx) \, \hnu(\D \bsx), \qquad \sum_{i=1}^n  w_i = \hnu(\cx) = 1.
\end{equation}
The sampling measure, $\hnu$, assigns a weight $w_i$ to the function value at the 
$i^{\text{th}}$ data site, and lies in the vector space
\begin{equation} \label{FJH:eq:sampmeaset}
\cm_{\textup{S}} := \left \{\sum_{i=1}^n w_i \delta_{\bsx_i} : w_1, \ldots, w_n \in \R,\ \bsx_1, 
\ldots, \bsx_n \in \calX, \ n \in \N \right \},
\end{equation}
where the $\delta_{\bst}$ denotes a Dirac measure concentrated at the point $\bst$.  
For now our interpretation of ``Monte Carlo'' is quite broad---the data sites and the 
weights may be deterministic or random.  Later, we impose further constraints.

This tutorial describes how to characterize and analyze the error, $\mu - \hmu$.  Specifically, we ask
\emph{
\begin{list}{}{\setlength\leftmargin{7ex}\setlength\labelwidth{5ex}}
\item[Question 1.] How do good sampling schemes, $\hnu$, make the error, $\mu - 
\hmu$, smaller?
\item[Question 2.] How can the error be decreased by re-casting the problem, say with a 
different $f$ or $\nu$?
\item[Question 3.] How many samples, $n$, are required to meet a specified error 
tolerance?
\end{list}}
\noindent The aim is to summarize some known---but perhaps not widely 
known---results.  We introduce and illustrate these results by examples.

\section{Big Data Survey} \label{FJH:sec:sleep} 
Did you get enough sleep last night?  Did you get more sleep than the average person?  
How much sleep did the average person get? The answer to this last question is simply 
$\mu = N^{-1} \sum_{j=1}^N f(j)$, where $f(j)$ denotes the hours of sleep of individual 
$j$ in a population of $N$ individuals.  For a large population, such as the $\approx 250$ 
million adults in the United States, collecting the data is difficult. So you might resort to a 
web survey, where you receive responses from individuals $x_1, \ldots, x_n$.  The error of 
your sample mean, $\widehat{\mu} = n^{-1} \sum_{i=1}^n f(x_i)$, may be expressed as a 
product of three quantities:
\begin{align}
\nonumber
\mu - \widehat{\mu} 
&= \frac{1}N \sum_{j=1}^N f(j) -  \frac 1n \sum_{i=1}^n f(x_i)  \\
\nonumber
&= - \frac{1}N \sum_{j=1}^N \bigl[ f(j) - \mu \bigr]\left ( \frac Nn \bbone_{\{x_i\}_{i=1}^n}(j) 
- 1 \right) \\
& = \underbrace{-\corr(f, \bbone_{\{x_i\}_{i=1}^n})}_{\algn^\Dt(f,\nu - \hnu)} \,
\underbrace{\sqrt{\frac{1 - n/N}{n/N}}}_{\disc^\Dt(\nu - \hnu)} \, 
\underbrace{\std(f)}_{\Var^\Dt(f)}.
\label{FJH:eq:surveydettrio}
\end{align}
Here, the population covariance of two functions $f$ and $g$, with respective 
population means $\mu(f)$ and $\mu(g)$, is defined as 
\begin{equation*}
\Cov(f,g) : = \frac 1N \sum_{j=1} [f(j) - \mu(f)] [g(j) - \mu(g)], \qquad 
\std(f) : = \sqrt{\Cov(f,f)},
\end{equation*}
and direct calculation yields
\begin{equation*}
\std\left(\frac Nn \bbone_{\{x_i\}_{i=1}^n}\right) = \sqrt{\frac{1 - n/N}{n/N}}.
\end{equation*}

The expression of the error as a product of three quantities in 
\eqref{FJH:eq:surveydettrio} is an case of the \emph{trio identity}, introduced by Meng 
\cite{Men16a}.  In this example the integral, $\mu$, corresponds to a sum, 
and the probability measure $\nu$ is the uniform measure over the sample space $\{1, 
\ldots, N\}$.  The sampling measure corresponding to the web survey is $\hnu = 
n^{-1} \sum_{i=1}^n \delta_{x_i}$.  The three terms that comprise the trio identity have the 
following meanings:
\begin{description}
	\item[$\Var(f)$] measures the \emph{variation} of the integrand from a typical value. 
	In 	this case, $\Var(f)$ corresponds to the standard deviation of $f$ and the typical 
	value	is the mean of $f$. The variation is positively homogeneous, i.e., $\Var(cf)  = 
	\abs{c} \Var(f)$.  The variation is \emph{not} the variance.
	\item [$\disc(\nu - \hnu)$] measures the \emph{discrepancy} of the sampling 
	measure 
	from the probability measure that defines the integral.  In this case, the discrepancy 
	depends only on the sample fraction, $n/N$.
	\item [$\algn(f,\nu - \hnu)$] measures the \emph{confounding} between the 
	integrand and the difference between the measure defining the integral and the 
	sampling measure.  In this case the confounding lies between plus and minus one.
\end{description}
The superscript $\Dt$ in \eqref{FJH:eq:surveydettrio} denotes a \emph{deterministic} 
version of the trio identity, which is defined in general in Sec.\ \ref{FJH:sec:dettrio}. 
Other versions of the trio identity are introduced later.

\begin{FJHLesson}
	\FJHLessonZero
\end{FJHLesson}

For the web survey example one might expect $\Var^\Dt(f)$ to be one or two hours.  It 
is an inherent quality of what we are measuring and is not affected by our sampling 
scheme.  The discrepancy in this example will be quite large unless our survey includes a 
significant proportion of the population.  One might hope to make  the confounding 
small.  This will happen if the set of individuals who complete the survey 
is only weakly correlated with the number of hours of sleep.  Unfortunately, those who  
complete web surveys might tend to be those who are not sleeping as much as the 
population average.  It is difficult to estimate how small the confounding might be.  

An alternative to the web survey is a simple random sample.  In this 
case, we may use a 
\emph{randomized} version of the trio identity, which is defined in general in Sec.\  
\ref{FJH:sec:rndtrio}.  Starting 
from \eqref{FJH:eq:surveydettrio}, the 
error of a sleep survey using a simple random sample without replacement is
\begin{align}
\nonumber
\mu - \widehat{\mu} 
& = -\corr(f, \bbone_{\{x_i\}_{i=1}^n}) \,
\sqrt{\frac{1 - n/N}{n/N}} \, 
\std(f) \\
& = \underbrace{\frac{- \corr\bigl(f, \bbone_{\{x_i\}_{i=1}^n}\bigr)}
{\std_{\hnu} \bigl( \corr\bigl(f, \bbone_{\{x_i\}_{i=1}^n}\bigr)\bigr)}}_{\algn^\Rn(f,\nu - \hnu)} 
\, 
\underbrace{\sqrt{\frac{1-n/N}{n(1-1/N)}}}_{\disc^\Rn(\nu - \hnu)} \, 
\underbrace{\std(f)}_{\Var^\Dt(f)},
\label{FJH:eq:surveyrndtrio}
\end{align}
where $\std_{\hnu}$ means standard deviation with respect to the random sampling 
scheme, and direct calculation yields
\begin{equation*}
\std_{\hnu} \bigl( \corr\bigl(f, \bbone_{\{x_i\}_{i=1}^n}\bigr)\bigr) = \sqrt{\frac{1}{N-1}}.
\end{equation*}

For the randomized trio identity, the confounding may now be greater than one in 
magnitude.  However, it confounding does have zero mean and 
standard deviation one.  This means that with probability $1-\alpha$, the magnitude of 
the confounding will be no greater than $1/\sqrt{\alpha}$ by Chebyshev's inequality. The 
sampling scheme generated by a web survey is in the sample space of the simple 
random survey, which is all possible choices of $n$ out of $N$ individuals.  Unless the 
web survey has a value of $\corr(f, \bbone_{\{x_i\}_{i=1}^n})$ 
on the order of $\Order(1/\sqrt{N})$, then it is one of the ``awful minority'' of 
sampling schemes.

While the randomized confounding may be larger than in the deterministic case, 
the randomized discrepancy is $\Order(1/\sqrt{N})$ than in the 
deterministic case.  Comparing \eqref{FJH:eq:surveyrndtrio} with 
\eqref{FJH:eq:surveydettrio}, we conclude that for a large population a web survey with 
$n_{\textup{web}}$ samples is equivalent to a simple random survey with 
\[
n_{\textup{SRS}} \approx \frac{n_{\textup{web}}/N}{\corr^2\Bigl(f, 
\bbone_{\{x_i\}_{i=1}^{n_{\textup{web}}}}\Bigr)(1-n_{\textup{web}}/N)}
\]
samples.  For example, an exceedingly successful web survey with $n_{\textup{web}} = 
1$ million distinct responses from a population of $N = 250$ million 
(approximately the US adult population) with a correlation as small as $0.05$ is about as 
valuable as $n_{\textup{SRS}} = 1.6$ simple random samples.

This example illustrates a danger with assigning too much importance to big data. Unless 
the 
sample fraction is close to one or the correlation between 
your sampling scheme and what you are measuring can be made tiny, a large quantity of 
data may give a worse answer than a small quantity of carefully collected data.

\begin{FJHLesson}
	\FJHLessonOne
\end{FJHLesson}

The trio identity, in the two forms that we have seen so far, begins to answer Question 1 
in the introduction.  The main way to choose a good sampling scheme is to choose one 
with small discrepancy, since this is the main measure of sample quality.   However, the 
confounding is also influenced by the choice of sampling scheme, and this term should 
not be ignored.

The initial answer to Question 2 in the introduction is that it may help to reduce the 
variation of the integrand.  This can be done, for example, by a change of variables.  An 
example of a useful variable transformation is provided in Section ??

\section{The Deterministic Trio Identity for Cubature Error} \label{FJH:sec:dettrio}
The two versions of the trio identity introduced by example in the previous section may 
be defined in generality. We begin with the deterministic version.  

The integrand is 
assumed to lie in some Banach space, $(\cf,\norm{\cdot}_{\cf})$, where function 
evaluation at any point  in the 
domain,  $\cx$, is a 
bounded, linear functional.  This means that $\sup_{f \in \cf} \abs{f(\bst)}/\norm{f}_{\cf} < 
\infty$ for all $\bst \in \cx$ and that $\int_{\cx} f(\bsx) \, \delta_{\bst}(\D\bsx) =f(\bst)$ 
for all $f \in \cf, \ \bst \in \cx$, where $\delta_{\bst}$ denotes the Dirac measure 
concentrated at $\bst$.  Let $L :\cf \to \R$ be some bounded 
linear functional.  If $L(f) \ne 0$ for any $f$, then $\cf$ is assumed to contain all 
constant functions.  The deterministic discrepancy is defined as 
\begin{equation}  \label{FJH:eq:detvardef}
\Var^{\Dt}(f) := \norm{f- L(f)}_{\cf} \qquad \forall f \in \cf.
\end{equation} 

Let $\cm$ denote the vector space of signed measures for which integrands in $\cf$ 
have finite integrals:
\begin{equation*}
\cm : = \biggl \{\text{signed measures } \eta : \int_{\cx} f(\bsx) \, \eta(\D\bsx) < \infty \ \ 
\forall f \in \cf \biggr \}.
\end{equation*}
We assume that $\nu \in \cm$ so that our integral of interest is defined.  Since function 
evaluation is bounded, $\cm$ includes $\cm_{\textup{S}}$ the vector space of all 
sampling measures.  Define the subspace  
\begin{equation} \label{FJH:eq:Mperpdef}
\cm_\bot : = \left \{\eta \in \cm :  L(f) \eta(\cx) = 0  \ \forall f \in \cf
\right\}.
\end{equation} 
A semi-norm on $\cm_\bot$ is induced by the norm on $\cf$, which provides the 
definition of discrepancy:
\begin{equation} \label{FJH:eq:detdiscdef}
\snorm{\eta}_{\cm_\bot}  : =\sup_{f \in \cf : f \ne 0} \frac{\displaystyle \biggabs{\int_{\cx} 
f(\bsx) \, \eta(\D \bsx)}}{\norm{f}_{\cf}}, \qquad \disc^\Dt(\nu - \hnu) : = \norm{\nu - 
\hnu}_{\cm_\bot}.
\end{equation}

Finally, define the confounding as 
	\begin{equation} \label{FJH:eq:detconfdef}
\algn^\Dt(f,\nu - \hnu): =  \begin{cases} \displaystyle 
\frac{\displaystyle\int_{\cx} f(\bsx) \, (\nu - \hnu)(\D 
	\bsx)}{\Var^\Dt(f)\disc^\Dt(\nu - \hnu)},  & 
\Var^\Dt(f)\disc^\Dt(\nu - \hnu) \ne 0, \\[2ex]
0, & \text{otherwise}.
\end{cases}
\end{equation}
The above definitions allow us to establish the deterministic trio identity for cubature 
error.

\begin{theorem}[Deterministic Trio Error Identity]  \label{FJH:thm:dtrio} For the spaces 
of integrands and 
measures defined above, and for the above definitions of variation, discrepancy, and 
confounding, the following error identity holds for all $f \in \cf$ and $\nu - \hnu  \in 
\cm_\bot$: 
\begin{equation} \tag{DTRIO} \label{FJH:eq:dtrio}
\mu - \hmu  = \algn^\Dt(f,\nu - \hnu) \, \disc^\Dt(\nu - \hnu) \, \Var^{\Dt}(f).
\end{equation}
Moreover, $\abs{\algn^\Dt(f,\nu - \hnu)} \le 1$. 
\end{theorem}
\begin{proof}  The proof of this identity follows directly from the definitions above, by 
noting that for all $f \in \cf$ and $\nu - \hnu  \in \cm_\bot$, the error can be written as a 
single integral via \eqref{INT} and \eqref{AppxINT}:
	\begin{equation} \label{FJH:eq:err_as_int}
	\mu - \hmu   =  \int_{\cx} f(\bsx) \, (\nu - \hnu)(\D \bsx).
	\end{equation}
	If $\Var^{\Dt}(f) = 0$, then $f = L(f)$, and the integral above vanishes due to the 
	definition of $\cm_{\bot}$.  If $\disc^\Dt(\nu - \hnu) = 0$, then the integral above 
	vanishes by \eqref{FJH:eq:detdiscdef}.  Thus, for $\Var^{\Dt}(f) \disc^\Dt(\nu - \hnu) = 
	0$ 
	the trio identity holds. If $\Var^{\Dt}(f) \disc^\Dt(\nu - \hnu) \ne 0$, then the trio 
	identity also holds by the definition of the confounding.
	
	Next, we analyze the magnitude of the confounding for $\Var^{\Dt}(f) \disc^\Dt(\nu - 
	\hnu) \ne 0$: 
	\begin{align*}
	\abs{\algn(f,\nu - \hnu)} & = 
		\frac{\biggl \lvert\displaystyle\int_{\cx} f(\bsx) \, (\nu - \hnu)(\D 
			\bsx) \biggr \rvert}{\Var^\Dt(f)\disc^\Dt(\nu - \hnu)} \quad \text{by 
			\eqref{FJH:eq:detconfdef}}\\
		& = \frac{\biggl \lvert\displaystyle\int_{\cx} [f(\bsx) - L(f)] \, (\nu - \hnu)(\D 
			\bsx) \biggr \rvert}{\norm{f-L(f)}_{\cf}\disc^\Dt(\nu - \hnu)} \quad \text{by 
			\eqref{FJH:eq:detvardef} and \eqref{FJH:eq:Mperpdef}} \\
		& \le 1 \quad \text{by \eqref{FJH:eq:detdiscdef}},
\end{align*}
since $\Var^{\Dt}(f) \ne 0$ and so $f - L(f) \ne 0$.
\end{proof}

Because the confounding has magnitude no greater than one, the deterministic trio 
identity may be converted into a deterministic error bound by 
ignoring the confounding:  $\abs{\mu - \hmu}  \le \disc^\Dt(\nu - \hnu) \, \Var^{\Dt}(f)$.  
Cubature error bounds of this form have a long history, dating back to the inequalities of 
Koksma \cite{Kok42} and Hlawka \cite{Hla61}. See also the monograph of Niederreiter 
\cite{Nie92}.  The confounding can be understood as the actual error divided by the 
absolute worst-case error.

For the big data sleep survey in Sec.\ \ref{FJH:sec:sleep}, $\cf$ is the space of all 
real-valued functions on $\cx := \{1, \ldots, N\}$ with scaled $\ell^2$ norm 
$\norm{f}_{\cf} 
:= N^{-1/2} \Bignorm{\bigl(f(j)\bigr)_{j=1}^N}_{\ell^2}$, and $L(f) := N^{-1}\sum_{j=1}^N f(j) 
= 
\mu $ is the population mean of $f$. It then follows that the variation of 
the integrand, $f$, is its standard deviation, as is seen in \eqref{FJH:eq:surveydettrio}.
The corresponding space of measures of interest is $\cm_\bot := \bigl \{ \sum_{i=1}^N u_j 
\delta_{j} : (u_1, \ldots, u_N) \in \R^N,\ u_1 + \cdots + u_N= 0 \bigr \}$, and its norm is 
$\bignorm{\sum_{i=1}^N u_i \delta_{i}}_{\cm_\bot} : = 
	\sqrt{N} \Bignorm{\bigl(u_i\bigr)_{i=1}^N}_{\ell^2}$.   The expressions for the 
	deterministic 
	discrepancy and confounding in \eqref{FJH:eq:surveydettrio} follow from these 
	choices.

When $\cf$ is a Hilbert space with reproducing kernel $K$, the discrepancy takes a 
simple, explicit form in terms of $K$.  The reproducing kernel is the unique function, $K: 
\cx \times \cx \to \R$ satisfying these two properties:
\begin{equation}
K(\cdot,\bst) \in \cf \quad \forall \bst \in \cx, \qquad f(\bst) = \ip[\cf]{K(\cdot,\bst)}{f} 
\quad \forall f \in \cf, \ \bst \in \cx.
\end{equation}
The Riesz Representation Theorem allows one to write the representer of cubature error 
as 
\begin{equation}
\eta_{\err}(\bst) = \ip[\cf]{K(\cdot,\bst)}{\eta_{\err}} = \int_{\cx} K(\bsx,\bst) \, (\nu - 
\hnu)(\D\bsx).
\end{equation}
Thus, 
\begin{equation}
\mu - \widehat{\mu} =  \ip[\cf]{\eta_{\err}}{f} = 
\underbrace{\frac{\ip[\cf]{\eta_{\err}}{f}}{\norm{f - 
L(f)}_{\cf} \norm{\eta_{\err}}_{\cf}} }_{\algn^\Dt(f,\nu - \hnu)}\, 
\underbrace{\norm{\eta_{\err}}_{\cf}}_{\disc^{\Dt}(\nu - \hnu)} \, 
\underbrace{\norm{f - L(f)}_{\cf}}_{\Var^\Dt(f)}
\end{equation}
provided that $\int_{\cx} L(f) \, (\nu - \hnu)(\D \bsx) = 0$.  The discrepancy  takes the 
form \cite{Hic99a}
\begin{align}
\nonumber
[\disc^{\Dt}(\nu - \hnu)]^2 & = \norm{\eta_{\err}}_{\cf}^2 = \ip[\cf]{\eta_{\err}}{\eta_{\err}} 
\\
\nonumber
& = \int_{\cx \times \cx} K(\bsx,\bst) \, (\nu - \hnu)(\D \bsx) \, (\nu - \hnu)(\D \bst) \\
\nonumber
& = \int_{\cx \times \cx} K(\bsx,\bst) \, \nu(\D \bsx) \, \nu (\D \bst)  \\
& \qquad \qquad - 2 \sum_{i=1}^n w_i 
\int_{\cx} K(\bsx_i,\bst) \, \nu(\D \bst)+ \sum_{i,j = 1}^n w_iw_jK(\bsx_i,\bsx_j).
\end{align}
The computational cost to evaluate the discrepancy is $\Order(n^2)$ times the cost of 
evaluating the reproducing kernel at a point. 

Note that in the reproducing kernel Hilbert space case, the confounding corresponds to 
the cosine of the angle between the integrand, $f$, and the representer of the error, 
$\eta_{\err}$.  This cosine is no greater than one in magnitude, as expected.

A particular example of this reproducing kernel Hilbert space setting corresponds to 
$\nu$ $v$ being the uniform probability measure on the $d$-dimensional unit cube, $\cx 
= 
[0,1]^d$, and the reproducing kernel defined by \cite{Hic97a}
\begin{equation} \label{FJH:eq:L2Kdef}
K(\bsx,\bst) =\prod_{k = 1}^d [2 - \max(x_k,t_k)],
\end{equation}
which is plotted in Fig.\ \ref{FJH:fig:L2ker}.  In this example, $L(f) = f(\bsone)$, and the 
variation is defined as
\begin{equation*}
\Var^\Dt(f)  = \norm{f-f(\bsone)}_\cf = \Bigl \lVert \bigl ( \norm{\partial^\fraku f}_{L^2}\bigr 
)_{\emptyset \subsetneq \fraku \subseteq 1:d} \Bigr \rVert_{\ell^2} , \qquad 
\partial^\fraku f : = \frac{\partial^{\lvert\fraku\rvert} f}{\partial \bsx_\fraku} \biggr 
\rvert_{\bsx_{\bar{\fraku}} = \bsone}.
\end{equation*}
Here $1\!:\!d : = \{1, \ldots, d\}$, $x_\fraku := (x_k)_{k \in \fraku}$, and $\bar{\fraku}$ 
denotes the complement of $\fraku$.  
The square discrepancy for the case where $w_i = 1/n$ is
\begin{align}
\nonumber
[\disc^\Dt(\nu - \hnu)]^2  &= \left(\frac 43\right)^d - \frac{2}n \sum_{i=1}^n \prod_{k=1}^d 
\left (\frac{3 - x_{ik}^2}{2} \right) + \frac{1}{n^2}\sum_{i,j=1}^n \prod_{k = 1}^d [2 - 
\max(x_{ik},x_{jk})] 
\\ &= \Bigl \lVert \bigl ( \norm{\nu([\bszero,\cdot_\fraku]) - 
	\hnu([\bszero,\cdot_\fraku])}_{L^2}\bigr )_{\emptyset \subsetneq \fraku \subseteq 1:d} 
	\Bigr 
	\rVert_{\ell^2}. \label{FJH:eq:L2disc}
\end{align}

This discrepancy has a geometric interpretation. Fig.\ \ref{FJH:fig:L2ker} displays the 
projections of some arbitrary 
data sites, $\bsx_{1,\fraku}, \ldots, \bsx_{n,\fraku}$, for $n=32$ and $\fraku = \{5,8\}$.  
Also 
shown is $\bsx_\fraku$, where $\bsx = (\ldots, 0.6, \ldots, 0.4, \ldots)$. The interval 
$[\bszero,\bsx_\fraku]$ has volume $0.24$ under the uniform measure, $\nu$, However, 
$[\bszero,\bsx_\fraku]$ only has 
volume $7/32 = 0.21875$ under the sampling measure, $\hnu$, corresponding to the 
proportion of data sites lying inside the interval $[\bszero,\bsx_\fraku]$. The difference is 
$0.02125$.  The average of this kind of difference, for all $\bsx \in [0,1]^d$ and for all 
$\emptyset \subsetneq \fraku 
\subseteq 1\!:\!d$ is what comprises the discrepancy above, which is called the 
$L^2$-discrepancy.

\begin{figure}
	\centering
	\includegraphics[height = \FJHfigheight]{ProgramsImages/L2Kernel.eps}\qquad
	\includegraphics[height = \FJHfigheight]{ProgramsImages/LocalDiscrep.eps}
	\caption{The left depicts the reproducing kernel in \eqref{FJH:eq:L2Kdef} for the 
	$L^2$-discrepancy 
	for $d=1$.  The right depicts the geometric interpretation of the discrepancy.
	\label{FJH:fig:L2ker}}
\end{figure}

If the data sites  $\bsx_{1}, \ldots, \bsx_{n}$ are chosen to be independent and identically 
distributed (IID) with probability measure $\nu$, and $w_1 = \cdots = w_n = 1/n$, then 
the mean square discrepancy for 
the reproducing Hilbert space case is
\begin{equation*}
\bbE \bigl\{[\disc^{\Dt}(\nu - \hnu)]^2 \bigr \}  = \frac 1 n \left [ \int_{\cx} K(\bsx,\bsx) \, 
\nu(\D 
\bsx) - 
\int_{\cx \times \cx} K(\bsx,\bst) \, \nu(\D \bsx) \, \nu (\D \bst) \right ].
\end{equation*}
For the $L^2$ discrepancy in \eqref{FJH:eq:L2disc} this becomes 
\begin{equation} \label{FJH:eq:IIDL2disc}
\bbE \bigl\{[\disc^{\Dt}(\nu - \hnu)]^2 \bigr \} = \frac 1 n \left [ \left(\frac 32\right)^d - 
\left(\frac 43\right)^d \right ].
\end{equation}

In all versions of the trio identity, the discrepancy is the key term that should converge to 
zero as the sample size tends to infinity.  The variation is independent of the sample 
size, and the confounding may decrease, increase or oscillate with increasing $n$.  From 
the formulas above, the convergence of the discrepancy to zero for IID sampling as $n 
\to 
\infty$ is  $\Order(1/\sqrt{n})$, which is consistent with the expected convergence of the 
cubature error for IID Monte Carlo.  From \eqref{FJH:eq:IIDL2disc} we see that the 
value of the discrepancy may depend strongly on the dimension, $d$.  We return 
to this matter in Sec.\ ???

Quasi-Monte Carlo methods choose the data sites $\{\bsx_i\}_{i=1}^n$ to be dependent 
and more evenly spread than IID points.  For integration over $\cx = [0,1]^d$ with 
respect to the uniform measure, the data sites may be chosen to be
\begin{itemize} 
\item a digital net \cite{DicPil10a}, such as that proposed by Sobol' \cite{}, Faure \cite{}, 
Niederreiter \cite{} or Niederreiter and Xing \cite{}, or 
\item a node set of an integration lattice \cite{SloJoe94}.  
\end{itemize}
For details on the 
constructions of such sets, see the references above and Pierre L'Ecuyer's tutorial 
\cite{}.  The evenly spread data sites are often called \emph{low discrepancy} sets 
because the discrepancy in \eqref{FJH:eq:L2disc} and its relatives is 
$\Order(1/n^{1-\epsilon})$ as $n \to \infty$ for any positive $\varepsilon$.  

Fig.\ \ref{FJH:fig:plotsdiffpts} displays examples of IID and low discrepancy data sites.  
Fig.\ \ref{FJH:fig:unwtdiscdiffpts} shows the decay of the discrepancies of these data 
sites for various dimensions.  Although the decay of the discrepancy for lattice nodesets 
and Sobol' points is $\Order(1/n^{1-\epsilon})$ for large enough $n$, the decay in Fig.\ 
\ref{FJH:fig:unwtdiscdiffpts} appears to be 
more like $\Order(1/\sqrt{n})$ for large dimensions and small $n$.   This dimension 
dependence of the convergence rate is addressed later in ???.  


\begin{FJHLesson}
	\FJHLessonTwo
\end{FJHLesson}


\begin{figure}
	\centering
	\includegraphics[height=\FJHfigheight]{ProgramsImages/IIDPoints.eps} \qquad
	\includegraphics[height=\FJHfigheight]{ProgramsImages/ShiftedLatticePoints.eps} \\
	\includegraphics[height=\FJHfigheight]{ProgramsImages/USobolPoints.eps} \qquad
	\includegraphics[height=\FJHfigheight]{ProgramsImages/SSobolPoints.eps}
	\caption{An illustration of IID points and three examples of low discrepancy points 
	\label{FJH:fig:plotsdiffpts}}
\end{figure}

\begin{figure}
	\centering
	  \includegraphics[height=\FJHfigheight]{ProgramsImages/UnwtL2DiscLat.eps}   
	  \qquad 
	  \includegraphics[height=\FJHfigheight]{ProgramsImages/UnwtL2Disc.eps} 
	\caption{The root mean square $L^2$ discrepancies given by \eqref{FJH:eq:L2disc} 
	for randomly shifted 
	lattice sequence nodesets and randomly scrambled and shifted Sobol' sequences 
	points.  A variety of dimensions are shown.
		\label{FJH:fig:unwtdiscdiffpts}}
\end{figure}

\section{The Randomized Trio Identity for Cubature Error} \label{FJH:sec:rndtrio}
For the randomized version of the trio identity, we again assume that the integrands lie in 
a Banach space, $(\cf,\norm{\cdot}_{\cf})$, and that this Banach space contains 
constant functions.  This time we do not require 
function evaluation to be a bounded linear functional on $\cf$, however it is assumed 
that $\int_{\cx} f(\bsx) \, \nu(\D\bsx)$ is defined  for all $f \in \cf$.  The definitions of the 
bounded linear functional $L$ and the variation in the deterministic case in  
\eqref{FJH:eq:detvardef} apply here as well.

Now endow, $\cm_{\textup{S}}$, the vector space of all sampling measures, with a 
probability distribution.  This means that the placement of the data sites, the number of 
data sites, and the choice of the weights may all be random.  We require that 
the following two conditions are satisfied:
\begin{subequations} \label{FJH:eq:randMcond}
\begin{gather}
\bbE_{\hnu} \biggabs{\int_{\cx} f(\bsx) \,  \hnu(\D \bsx)}^2 < \infty \qquad \forall f \in \cf, \\
\label{FJH:eq:randMcondB}
L(f) [\nu(\calX) - \hnu(\calX)] = 0  \qquad \text{almost surely}.
\end{gather}
\end{subequations}
The first of these conditions implies that $\int_{\cx} f(\bsx) \,  \hnu(\D \bsx)$ exists 
almost surely for every $f \in \cf$.  

The randomized discrepancy is  defined as the worst  normalized root mean squared 
error:
\begin{equation} \label{FJH:eq:rnddiscdef}
\disc^\Rn(\nu - \hnu) : =\sup_{f \in \cf : f \ne 0} \frac{\displaystyle \sqrt{\bbE_{\hnu}
\biggabs{\int_{\cx} 
		f(\bsx) \, (\nu - \hnu)(\D \bsx)}^2}}{\norm{f}_{\cf}}.
\end{equation}
The randomized discrepancy does not depend on the particular instance of the 
sampling measure but on the distribution of the sampling measure. 

Finally, define the confounding as 
\begin{equation} \label{FJH:eq:rndconfdef}
\algn^\Rn(f,\nu - \hnu): =  \begin{cases} \displaystyle 
\frac{\displaystyle\int_{\cx} f(\bsx) \, (\nu - \hnu)(\D 
	\bsx)}{\Var^\Dt(f)\disc^\Rn(\nu - \hnu)},  & 
\Var^\Dt(f)\disc^\Rn(\nu - \hnu) \ne 0, \\[2ex]
0, & \text{otherwise}.
\end{cases}
\end{equation}
The above definitions allow us to establish the randomized trio identity for cubature 
error.

\begin{theorem}[Randomized Trio Error Identity]  \label{FJH:thm:rtrio} For the spaces of 
integrands and 
	measures defined above, and for the above definitions of variation, discrepancy, and 
	confounding, the following error identity holds for all $f \in \cf$ and $\hnu  \in 
	\cm_{\textup{S}}$: 
	\begin{equation} \tag{RTRIO} \label{FJH:eq:rtrio}
	\mu - \hmu  = \algn^\Rn(f,\nu - \hnu) \, \disc^\Rn(\nu - \hnu) \, \Var^{\Dt}(f) \quad 
	\text{almost surely}.
	\end{equation}
	Moreover, $\Ex_{\hnu} \abs{\algn^\Rn(f,\nu - \hnu)}^2  \le 1$ for all $f \in 
	\cf$. 
\end{theorem}
\begin{proof}  For all $f \in \cf$ and $\hnu  \in \cm_{\textup{S}}$, the error can be 
written as the single integral in \eqref{FJH:eq:err_as_int} almost surely. 	If $\Var^{\Dt}(f) 
= 0$, then $f = L(f)$, and $\mu - \hmu$ vanishes almost surely due to condition 
\eqref{FJH:eq:randMcondB}.  If 
$\disc^\Rn(\nu - \hnu) = 0$, then $\mu - \hmu$
	vanishes almost surely by \eqref{FJH:eq:rnddiscdef}.  Thus, for $\Var^{\Dt}(f) 
	\disc^\Rn(\nu - \hnu) = 
	0$ 
	the trio identity holds. If $\Var^{\Dt}(f) \disc^\Rn(\nu - \hnu) \ne 0$, then the trio 
	identity also holds by the definition of the confounding.
	
	Next, we analyze the magnitude of the confounding for $\Var^{\Dt}(f) \disc^\Dt(\nu - 
	\hnu) \ne 0$: 
	\begin{align*}
	\bbE \abs{\algn^\Rn(f,\nu - \hnu)}^2 & = 
	\frac{\bbE \biggl \lvert\displaystyle\int_{\cx} f(\bsx) \, (\nu - \hnu)(\D 
		\bsx) \biggr \rvert^2 }{[\Var^\Dt(f)\disc^\Dt(\nu - \hnu)]^2} \quad \text{by 
		\eqref{FJH:eq:rndconfdef}}\\
	& = \frac{\bbE \biggl \lvert\displaystyle\int_{\cx} [f(\bsx) - L(f)] \, (\nu - \hnu)(\D 
		\bsx) \biggr \rvert^2}{[\norm{f-L(f)}_{\cf}\disc^\Dt(\nu - \hnu)]^2} \quad \text{by 
		\eqref{FJH:eq:detvardef} and \eqref{FJH:eq:randMcondB}} \\
	& \le 1 \quad \text{by \eqref{FJH:eq:rnddiscdef}},
	\end{align*}
	since $\Var^{\Dt}(f) \ne 0$ and so $f - L(f) \ne 0$.
\end{proof}

Consider simple Monte Carlo, where the approximation to the integral is an equally 
weighted average using IID sampling $\bsx_1, \bsx_2, \ldots \sim 
\nu$ without 
replacement. Let the sample size be fixed at $n$.
Let $\cf = L^{2,\nu}$, the space of functions that are square integrable with respect to 
the measure $\nu$, and let $L(f)$ be the mean of $f$.  Then the variation of $f$ is just 
its standard 
deviation, $\std(f)$.  The randomized discrepancy is 
$1/\sqrt{n}$, which allows for a random sample size.  The randomized 
confounding is 
\begin{align*}
\algn^\Rn(f,\nu - \hnu) = \frac{-1}{ \sqrt{n}\, \std(f)}    \sum_{i=1}^n [f(\bsx_i) - 
\mu].
\end{align*}
This is similar to the randomized trio identity for the big data survey in 
\eqref{FJH:eq:surveyrndtrio}.

There is no simple form for the randomized discrepancy for arbitrary sampling schemes 
and reproducing kernel Hilbert spaces like there is in the deterministic setting.  The 
randomized discrepancy can sometimes be conveniently calculated or bounded for 
spaces of integrands that are represented by series expansions, and the randomized 
sampling schemes for the bases of these expansions have special properties.  See ??? 
for examples.

It is instructive to contrast the variation, discrepancy, and confounding in the 
deterministic and randomized settings.   For some integrand, $f$, and some sampling 
measure, $\hnu$, satisfying the 
conditions defining both \eqref{FJH:eq:dtrio}  and \eqref{FJH:eq:rtrio}:
\begin{itemize}
	\item the variation in both settings is the same,
	
	\item the randomized discrepancy 
	must be no greater than the deterministic discrepancy by definition, and thus
	
	\item the randomized confounding must be no less than the deterministic 
	confounding.  
\end{itemize}
Theorem \ref{FJH:thm:dtrio} states that the deterministic confounding is never greater 
than one in magnitude.  By contrast, Theorem \ref{FJH:thm:rtrio} 
only states that the expected value of the magnitude of the randomized confounding is 
no greater than one.  The randomize confounding may be arbitrarily large, but Markov's 
inequality implies that it may be larger than $1/\sqrt{\alpha}$ with  probability no greater 
than $\alpha$.  The big data survey example illustrated the differences in the 
deterministic and randomized trio identities.  The next example illustrates this further.

\section{Multivariate Gaussian Probabilities} \label{FJH:sec:Gauss}
Consider the $d$-variate integral corresponding to the probability of a 
$\cn(\bszero,\mathsf{\Sigma})$ random variable lying inside the box $[\bsa,\bsb]$:
\begin{equation} \label{FJH:eq:MVN}
\mu = \int_{[\bsa,\bsb]} \frac{\exp\bigl(- \frac 12 \bsz^T \mathsf{\Sigma}^{-1} \bsz 
\bigr)}{\sqrt{(2 \pi)^d \det(\mathsf{\Sigma})}} \, \D \bsz = \int_{[0,1]^{d-1}} f(\bsx) \, \D 
\bsx,
\end{equation}
where $\mathsf{\Sigma} = \mathsf{L}\mathsf{L}^T$ is the Cholesky decomposition of 
the covariance matrix, $\mathsf{L} = \bigl(l_{jk}\bigr)_{j,k=1}^d$, is a lower triangular 
matrix, and
\begin{align}
\nonumber
\alpha_1 & = \Phi(a_1), \qquad \beta_1  = \Phi(b_1), \\
\nonumber
\alpha_j(x_1, \ldots, x_{j-1}) &= \Phi\left(\frac{1}{l_{jj}} \left(a_j - \sum_{k=1}^{j-1} 
l_{jk}\Phi^{-1}(\alpha_k + x_k(\beta_k-\alpha_k))\right)\right), \ j =2, \ldots, d,\\
\nonumber
\beta_j(x_1, \ldots, x_{j-1}) &= \Phi\left(\frac{1}{l_{jj}} \left(b_j - \sum_{k=1}^{j-1} 
l_{jk}\Phi^{-1}(\alpha_k + x_k(\beta_k-\alpha_k))\right)\right), \ j =2, \ldots, d, \\
f_{\textup{Genz}}(\bsx) &= \prod_{j=1}^{d} [\beta_j(\bsx) - \alpha_j(\bsx)]. 
\label{FJH:eq:Genz}
\end{align}
Genz \cite{Gen93} developed the clever transformation of variables above that turns the 
multivariate Gaussian probability into an integral over the unit cube.  Not only is the 
dimension decreased by one, but the integrand is typically made less peaky and more 
favorable to cubature methods.

The left plot of Fig.\ \ref{FJH:fig:MVNfig} shows the absolute errors in computing the the 
multivariate 
Gaussian probability via the Genz transformation for 
\[
   \bsa  = \begin{pmatrix}
   -6 \\ -2 \\ -2
   \end{pmatrix}, \quad
      \bsb  = \begin{pmatrix}
   5 \\ 2 \\ 1
   \end{pmatrix}, \quad
   \mathsf{\Sigma} = \begin{pmatrix} 16 & 4 & 4 \\ 4 &  2 &  1.5 \\
  4 & 1.5 &  1.3125 \end{pmatrix}, \quad
   \mathsf{L} = \begin{pmatrix} 4 & 0 & 0 \\ 1 &  1 &  0 \\
1 & 0.5 &  0.25 \end{pmatrix}, 
\]
by IID sampling, unscrambled Sobol' sequences, and scrambled Sobol' sequences.   
Multiple random scramblings of a very large scrambled Sobol' set was used to infer that 
$\mu \approx 0.6763373243578$.   For the two randomized sampling schemes $100$ 
replications were taken.  The marker denotes the median error and the tip of the stem 
extending above the marker denotes the worst $90\%$ error.

\begin{figure}
	\centering
	\includegraphics[height = \FJHfigheight]{ProgramsImages/MVNIIDUSobolSobol.eps} 
	\qquad 
	\includegraphics[height = \FJHfigheight]{ProgramsImages/MVNSobolGenzAff.eps}
	\caption{The error of an example of the multivariate Gaussian probability in 
	\eqref{FJH:eq:MVN}.  The left side shows the result of Genz's transformation and 
	different sampling schemes.  The right side shows the scrambled Sobol' sampling 
	using different transforamtions.
	\label{FJH:fig:MVNfig}}
\end{figure}

The error for IID sampling decays like $\Order(1/\sqrt{n})$, which is the same rate of 
decay of the randomized discrepancy for IID sampling, as mentioned in the previous 
section.  The error for  Sobol' sampling 
decays at the $\Order(1/n^{1-\epsilon})$ rate corresponding to the deterministic $L^2$
discrepancy defined in \eqref{FJH:eq:L2disc}.  

The error for randomly scrambled Sobol' 
sampling decays at a rate of roughly $\Order(1/n^{3/2- \epsilon})$, which corresponds to 
the randomized discrepancy for this sampling scheme \cite{???}.  Thus, not only do we 
see how the more evenly spaced Sobol' points improve upon IID sampling, but 
randomizing the Sobol' points while preserving their even structure provides further 
improvement to the rate of decay of the error.

Since unscrambled Sobol' sampling is one element in the space of sampling measures,
$\calM_{\textup{S}}$, how can one explain the poor decay of the cubature error with 
respect to the other sampling measures  in  $\calM_{\textup{S}}$.  Applying the 
randomized trio identity \eqref{FJH:eq:rtrio} to  unscrambled 
Sobol' sampling, it appears that although $\disc^\Rn(\nu - \hnu) = \Order(1/n^{3/2- 
\epsilon})$, it must be true that  $\algn^\Rn(f,\nu - \hnu) = \Order(1/n^{1/2- \epsilon})$, 
which is huge  in comparison to most sampling measures in
$\calM_{\textup{S}}$. 

\begin{FJHLesson}
\FJHLessonThree
\end{FJHLesson}

An alternative to the Genz transformation above is an affine transformation to compute 
the multivariate Gaussian probability:
\begin{gather}
\label{FJH:eq:aff}
\bsz = \bsa + (\bsb-\bsa) \circ \bsx, \quad f_{\textup{aff}}(\bsx) =  \frac{\exp\bigl(- 
\frac 12 \bsz^T
\mathsf{\Sigma}^{-1} \bsz 
	\bigr)}{\sqrt{(2 \pi)^d \det(\mathsf{\Sigma})}} \, \prod_{j=1}^d (b_j - a_j), \\
\nonumber
\mu = 
	\int_{[0,1]^{d}} f_{\textup{aff}}(\bsx) \, \D \bsx,
\end{gather} 
where $\circ$ denotes the Hadamard (term-by-term) product.  The right plot in Fig.\ 
\ref{FJH:fig:MVNfig} shows that the error using the affine 
transformation is much worse than that using the Genz transformation even though the 
two convergence rates are the same.  Fig.\ \ref{FJH:fig:GenzAfffig} shows the two 
integrands.  The difference in the magnitudes of the errors is 
mainly because $\Var^{\Dt}(f_{\textup{aff}})$ is greater than 
$\Var^{\Dt}(f_{\textup{Genz}})$.  This illustrates another 
lesson that provides a partial answer to Question 2 in the introduction.

\begin{figure}
	\centering
	\includegraphics[height = \FJHfigheight]{ProgramsImages/GenzFun.eps} \qquad 
	\includegraphics[height = \FJHfigheight]{ProgramsImages/AffineFun.eps}
	\caption{Two different integrands that can be used to compute multivariate normal 
	probabilities.  The left integrand is defined in \eqref{FJH:eq:Genz}, and the right 
	integrand is defined in  \eqref{FJH:eq:aff}.
		\label{FJH:fig:GenzAfffig}}
\end{figure}


\begin{FJHLesson} \label{FJH:eq:lessonfour}
	\FJHLessonFour
\end{FJHLesson}


\section{Option Pricing}
The prices of financial derivatives can often be modeled by high dimensional integrals.  If 
the underlying asset is described in terms of a Brownian motion, $B$, at times $t_1, 
\ldots, t_d$, then $\bsZ = (B(t_1), \ldots, B(t_d)) \sim \cn(\bszero, \mathsf{\Sigma})$, 
where $\mathsf{\Sigma}  = \bigl( \min(t_j,t_k) \bigr)_{j,k=1}^d$, and the fair price of the 
option is
\begin{equation*}
\mu = \int_{\R^d} \textup{payoff}(\bsz) \, \frac{\exp\bigl(- \frac 12 \bsz^T 
\mathsf{\Sigma}^{-1} 
\bsz 
\bigr)}{\sqrt{(2 \pi)^d \det(\mathsf{\Sigma})}} \, \D \bsz = \int_{[0,1]^{d}} f(\bsx) \, \D \bsx 
\end{equation*}
where the function $\textup{payoff}(\cdot)$ describes the discounted payoff of the 
option, 
\begin{equation*}
f(\bsx) = \textup{payoff}(\bsz), \qquad \bsz = \mathsf{L}^T \begin{pmatrix}
\Phi^{-1}(x_1) \\ \vdots \\ \Phi^{-1}(x_d)
\end{pmatrix}, 
\end{equation*}
and $\mathsf{L}$ is any square matrix satisfying $\mathsf{\Sigma} = \mathsf{L} 
\mathsf{L}^T$.


Fig.\ \ref{FJH:fig:AsianOpt} shows the cubature error using IID sampling, unscrambled 
Sobol' sampling, and scrambled Sobol' sampling for the Asian arithmetic mean call option 
with the following parameters:
\begin{gather*}
\textup{payoff}(\bsz) = \max \left( \frac 1d \sum_{j=1}^d S_j - K, 0\right)\E^{-rT}, \quad
S_j = S_0 \exp\bigl( (r - \sigma^2/2) t_j + \sigma z_j\bigr),
\\
T = 1, \quad d = 12, \quad S_0 = K = 100, \quad r =  0.05, \quad \sigma = 0.5, \\
t_j = jT/d, \ j = 1, \ldots, d
\end{gather*}
The convergence rate for IID and unscrambled Sobol' sampling are the same as in Fig.\ 
\ref{FJH:fig:MVNfig} for the previous example of multivariate probabilities.  However, 
although scrambling the Sobol' set improves the accuracy, it does not improve the 
convergence rate. 


\begin{figure}
	\centering
		\includegraphics[height = \FJHfigheight] 
		{ProgramsImages/AsianCallIIDUSobolSobol.eps} 
		\qquad
		\includegraphics[height = \FJHfigheight] 
		{ProgramsImages/AsianCallSobolPCADiff.eps}
		\caption{Cubature error for the price of an Asian arithmetic mean option using 
		different sampling 
		schemes. \label{FJH:fig:AsianOpt}}
\end{figure}

The convergence rate for scrambled Sobol' sampling, $\hnu$,  is poorer than hoped for 
because 
$f$ is not smooth enough for $\Var^{\Dt}(f)$ to be finite in the case 
where $\disc^\Rn(\nu - \hnu) = \Order(N^{-3/2 +\epsilon})$.  Less 
stringent conditions on $f$ are required for $\disc^\Dt(\nu - \hnu) = 
\Order(N^{-1 +\epsilon})$.  Even these requirements cannot 
be ensured for option pricing examples like that above except by a rather delicate 
argument \cite{GriKuoSlo10, GriKuoSlo16}.

\begin{FJHLesson}
	\FJHLessonFive
\end{FJHLesson}

The left plot in Fig.\ \ref{FJH:fig:MVNfig} chooses $\mathsf{L} = 
\mathsf{V}\mathsf{\Lambda}^{1/2}$, where the columns of $\mathsf{V}$ are the 
eigenvectors of $\mathsf{\Sigma}$, and the diagonal elements of the diagonal matrix  
$\mathsf{\Lambda}$ are the eigenvalues of $\mathsf{\Sigma}$.  This is also called a 
principal component analysis (PCA) construction.  The advantage is that the main part of 
the 
Brownian motion affecting the option payoff is concentrated in the smaller dimensions.  
The right plot of Fig.\ 
\ref{FJH:fig:MVNfig} contrasts the cubature error for two choices of  $\mathsf{L}$:  one 
chosen by the PCA construction and the other coming from the Cholesky decomposition 
of $\mathsf{\Sigma}$.  This latter choice corresponds to constructing the Brownian 
motion by time 
differencing.  The Cholesky decomposition of $\mathsf{\Sigma}$ gives poorer rate of 
convergence, illustrating again Lesson \ref{FJH:eq:lessonfour}.


\section{Bayesian Versions of the Trio Error Identity}
An alternative to a deterministic integrand, as has been assumed so far, is to assume an 
integrand that is a stochastic process.  Random input functions have been assumed by 
Diaconis \cite{Dia88a}, O'Hagan \cite{OHa91a}, Ritter \cite{Rit00a}, Rasmussen and 
Ghahramani \cite{RasGha03a}, and others. Specifically, suppose that $f \sim \GP (0, 
s^2C_{\bstheta})$, a zero mean Gaussian process.  The covariance of 
this  Gaussian process is $s^2C_{\bstheta}$, where $s$ is a scale parameter, and 
$C_{\bstheta}:\cx \times \cx \to \R$ is defined by a shape parameter $\bstheta$.  The 
sample space for this Gaussian process, $\calF$, does not enter significantly into the 
analysis.  Define the vector space of measures 
\begin{equation*}
\cm = \left\{\eta :  \int_{\calX^2} C_\bstheta(\bsx,\bst) 
\, \eta(\D \bsx) \eta(\D \bst) < \infty, \ \ \int_{\calX^2} C_\bstheta(\bsx,\bst) 
\, \eta(\D \bsx) < \infty \ \ \forall \bst \in \cx \right \},
\end{equation*}
and let $C_{\bstheta}$ be such that $\cm$ contains both $\nu$ and the Dirac measures 
$\delta_{\bst}$ for all $\bst \in \cx$. 

For a 
deterministic sampling measure, $\hnu = \sum_{i=1}^n w_i \delta_{\bsx_i}$, the cubature 
error, $\mu - \hmu$, is distributed as $\cn \bigl(0 , s^2(c_0 - 
2 \bsc^T \bsw + \bsw ^T \mC \bsw) \bigr)$ \cite{??}, where 
\begin{gather*}
c_0  = \int_{\calX^2} C_\bstheta(\bsx,\bst) \, \nu(\D \bsx) \nu(\D \bst), \qquad \bsc = 
\biggl( 
\int_{\cx} 
C_\bstheta(\bsx_i,\bst) \,\nu(\D \bst) \biggr)_{i=1}^n, \\
\mC  = \bigl( C_\bstheta(\bsx_i,\bsx_j) \bigr)_{i,j=1}^n, \qquad \bsw = \bigl(w_i 
\bigr)_{i=1}^n.
\end{gather*}
The dependence of $c_0$, $\bsc$, and $\mC$ on $\bstheta$ is suppressed in the 
notation for simplicity. We 
define the Bayesian variation and discrepancy as 
\begin{subequations} \label{FJH:eq:btriodef}
\begin{gather}
\label{FJH:eq:btriodefa}
\Var^{\Ba}(f)  = s, \qquad \disc^{\Ba}(\nu - \hnu) = \sqrt{c_0 - 
	2 \bsc^T \bsw + \bsw ^T \mC \bsw},\\
\algn^{\Ba}(f,\nu - \hnu) :=\frac{ \int_{\cx} 
	f(\bsx) \, (\nu - \hnu)(\D\bsx)}{s \sqrt{c_0 - 2 \bsc^T \bsw + \bsw ^T \mC \bsw}}
\end{gather}
\end{subequations}

\begin{theorem}[Bayesian Trio Error Identity]  \label{FJH:thm:btrio} Let the integrand be 
an instance of a zero mean Gaussian process with covariance $s^2C_{\bstheta}$ and 
that is drawn from a sample space 
$\calF$.  For the  variation, discrepancy, and 
	confounding defined in \eqref{FJH:eq:btriodef}, the following error identity holds: 
	\begin{equation} \tag{BTRIO} \label{FJH:eq:btrio}
	\mu - \hmu  = \algn^\Ba(f,\nu - \hnu) \, \disc^\Ba(\nu - \hnu) \, \Var^{\Ba}(f) \quad 
	\text{almost surely}.
	\end{equation}
	Moreover, $\algn^\Ba(f,\nu - \hnu) \sim \cn(0,1)$. 
\end{theorem}
\begin{proof}  Although $\int_{\cx} f(\bsx) \, \nu(\D \bsx)$ and $f(\bst) = \int_{\cx} 
f(\bsx) \, \delta_\bst (\D \bsx)$ may not exist for all $f \in \cf$, these two quantities exist 
almost surely because $\bbE_f [\int_{\cx} f(\bsx) \, \nu(\D \bsx)]^2 = c_0$, and 
$\bbE_f [f(\bsx)]^2 = C(\bsx,\bsx)$ are both well defined and finite.

The proof of the Bayesian trio identity follows directly from the definitions above.  The 
distribution of the confounding follows from the distribution of the cubature error.
\end{proof}

So far, the cubature weights have been kept arbitrary.  The choice of weights that 
minimizes the Bayesian discrepancy in \eqref{FJH:eq:btriodefa} is $\bsw = 
\mathsf{C}^{-1} \bsc$, which results in $\disc^{\Ba}(\nu - \hnu) = \sqrt{c_0 - \bsc ^T 
\mC^{-1} \bsc}$ and $\mu - \hmu \sim \cn \bigl(0 , s^2(c_0 - \bsc ^T 
\mC^{-1} \bsc) \bigr)$.  However, computing the weights requires $\Order(n^3)$ 
operations 
unless $\mathsf{C}$ has some special structure.  This computational cost is significant 
and may be a deterrent to the use of optimal weights unless the weights are 
precomputed. For smoother covariance functions, $C_\bstheta$, there is often a 
challenge of $\mathsf{C}$ being ill-conditioned.

The \emph{conditional} distribution of the cubature error, $\mu - \hmu$, given the 
observed data $ \{f(\bsx_i )= y_i\}_{i=1}^n$ is $\cn \bigl( \bsy^T (\mC^{-1}\bsc - 
\bsw) , s^2(c_0 - \bsc ^T \mC^{-1} \bsc) \bigr)$.  To remove the bias one should again 
choose $\bsw = \mathsf{C}^{-1} \bsc$.  This also makes the conditional distribution of 
the cubature error the same as the unconditional distribution of the cubature error.

The fact that the cubature error is a normal random variable allows us to use function 
values perform useful inference, namely, 
\begin{equation} \label{FJH:eq:MLEBdOne}
\Prob_f\bigl[\abs{\mu - \hmu} \le 2.58 
\disc^{\Ba}(\nu - \hnu) \Var^{\Ba}(f)\bigr] = 99\%.
\end{equation}
However, unlike our use of random sampling measures that are drawn using carefully 
crafted random number generators, there is no assurance that our integrand is actually 
drawn from a Gaussian process whose covariance we have fixed in advanced.  

The covariance, $s^2 C_{\bstheta}$, should be estimated, and one way to do so is to use 
maximum likelihood estimation (MLE), using the function values drawn for the purpose of 
estimating the integral.  The log-likelihood function for the data $ \{f(\bsx_i )= 
y_i\}_{i=1}^n$ is the probability density
\begin{equation*}
\ell(s,\bstheta | \bsy) = - \frac 12 s^2 \bsy^T \mathsf{C_{\bstheta}}^{-1} \bsy -\frac 12  
\log \bigl(
\det(\mathsf{C_{\bstheta}}) \bigr) - \frac n2 \log(s^2) + \text{constants}.
\end{equation*}
Maximizing with respect to $s$, yields 
\[
s_\MLE =  \sqrt{\frac 1n \bsy^T \mathsf{C_{\bstheta_\MLE}}^{-1} \bsy}
\]
Plugging this into the log likelihood leads to 
\[
\bstheta_\MLE =  \argmin_\bstheta \left[\frac 1n \log \bigl(\det(\mathsf{C_{\bstheta}}) 
\bigr) 
+ \log\bigl(\bsy^T \mathsf{C_{\bstheta}}^{-1} \bsy \bigr)  \right ]
\]
Using MLE estimates, the probabilistic error 
bound in \eqref{FJH:eq:MLEBdOne} becomes
\begin{multline} \label{FJH:eq:MLEBdTwo}
\Prob_f\left[\abs{\mu - \hmu} \le 2.58 
\sqrt{\frac 1n \left(c_{0,\bstheta_\MLE} - \bsc_{\bstheta_\MLE} ^T 
	\mC_{\bstheta_\MLE}^{-1} \bsc_{\bstheta_\MLE} \right)  \left(\bsy^T 
	\mathsf{C}_{\bstheta_\MLE}^{-1} \bsy\right)}\right] \\
 = 
99\%.
\end{multline}
Note that the value of $\bstheta_\MLE$ and the above Bayesian cubature error bounds  
is unchanged by replacing  $C_\bstheta$ by a positive multiple of itself.  

Let's revisit the multivariate normal probability example of Section \ref{FJH:sec:Gauss}, 
and perform Bayesian cubature with a covariance kernel with modest smoothness from 
the Mat\'ern  family:
\begin{equation} \label{FJH:eq:Matern}
C_\theta(\bsx,\bst) = \prod_{j=1}^d \left(1 + 
\theta\abs{x_j-t_j}\right) \exp\left(-\theta\abs{x_j-t_j}\right)
\end{equation}
A two-dimensional example of this covariance kernel is plotted in Fig.\ 
\ref{FJH:fig:MVNcubMLE}.


\begin{figure}
	\centering
		\includegraphics[height = \FJHfigheight] 
{ProgramsImages/Matern.eps} 
\\[2ex]
\includegraphics[height = \FJHfigheight] 
{ProgramsImages/MVNIIDUSobolSobolWtSobol.eps}
\qquad
\includegraphics[height = \FJHfigheight] 
{ProgramsImages/MVNSobolWtSobolErrBd.eps}
\caption{Cubature error for the price of an Asian arithmetic mean option using 
	different sampling 
	schemes. \label{FJH:fig:MVNcubMLE}}
\end{figure}

%\item $\disc^\Ba(\nu - \hnu) =  \disc^\Dt(\nu - \hnu)$  if  $C_{\vtheta} = K$	




\textbf{Continue here}


\section{Dimension Dependence of the Cubature Error}

Tractability



\begin{figure}
	\centering
	\includegraphics[height=\FJHfigheight]{ProgramsImages/WtL2DiscLat.eps}   \qquad 
	\includegraphics[height=\FJHfigheight]{ProgramsImages/WtL2Disc.eps} 
	\caption{The root mean square weighted $L^2$ discrepancies given by 
	\eqref{FJH:eq:wtL2disc} 
		for randomly shifted 
		lattice sequence nodesets and randomly scrambled and shifted Sobol' sequences 
		points.  A variety of dimensions are shown.
		\label{FJH:fig:wtdiscdiffpts}}
\end{figure}

\section{Variable Transformations}

\section{Adaptive Cubature}
Via Bayesian approach

Via our approach


\section{Discussion}
\FJHLessonZero \FJHLessonOne \FJHLessonTwo \FJHLessonThree \FJHLessonFour 
\FJHLessonFive



Various ideas
\begin{itemize}
\item series spaces
\item complexity bounds
\item tractability
\item multilevel methods
\item multivariate decomposition method, new Frances paper
\end{itemize}

\bibliographystyle{spmpsci}
\bibliography{FJH23,FJHown23}

\appendix
\section{Appendix}
\subsection{Gaussian Processes}
Suppose that $f:\cx \to \R$ is a Gaussian process with mean $m$ and covariance kernel $C(\cdot,\cdot)$, i.e., $f \sim \GP(m,C)$.  For fixed $\{\bsx_i\}_{i=1}^n$, the joint probability density function of $\bigl(f(\bsx_i)\bigr)_{i=1}^n$ is $\varrho$, defined as 
\begin{equation*}
\varrho (\bsy) = \frac{\exp\bigl(-\frac 12 (\bsy - m \bsone)^T \mC^{-1} (\bsy - m \bsone) \bigr)}{\sqrt{(2 \pi)^n \det(\mC)}}, \quad \bsy \in \R^n, \qquad \mC = \bigl(C(\bsx_i,\bsx_j)\bigr)_{i,j=1}^n.
\end{equation*}
Furthermore, the joint probability density function of $\bigl(\int_{\cx} f(\bsx) \, (\nu -\hnu)(\D \bsx), f(\bsx_1), \ldots, f(\bsx_n)\bigr)$ is $\tvarrho$, defined as 
\begin{align*}
\tvarrho (\tbsy) &= \frac{\exp\bigl(-\frac 12 (\tbsy - m \tbsone)^T \tmC^{-1} (\tbsy - m 
\tbsone) \bigr)}{\sqrt{(2 \pi)^{n+1} \det(\tmC)}}, \quad \tbsy = (y_i)_{i=0}^n \in \R^{n+1}, \\
 \tmC &= \begin{pmatrix} c_0 & \bsc^T \\ \bsc & \mC \end{pmatrix}, \qquad \tbsone = \bigl(1 - \bsone^T\bsw,  1, \ldots, 1 \bigr)^T, \\
 c_0 &= \int_{\cx^2} C(\bsx,\bst) \, (\nu -\hnu)(\D\bsx)(\nu -\hnu)(\D\bst) \\ 
 & = \int_{\cx^2} C(\bsx,\bst) \, \nu (\D\bsx)\nu(\D\bst) - 2 \sum_{i=1}^n w_i \int_{\cx} C(\bsx_i,\bsx) \, \nu (\D\bsx) +  \sum_{i,j=1}^n w_i w_j C(\bsx_i,\bsx_j)\\
 & = \tc_0 -2 \tbsc^T \bsw + \bsw^T \mC \bsw,\\
 & \qquad \qquad   \tc_0 = \int_{\cx^2} C(\bsx,\bst) \, \nu (\D\bsx)\nu(\D\bst), \qquad 
 \tbsc = \biggl( \int_{\cx} C(\bsx_i,\bsx) \, \nu (\D\bsx)\biggr)_{i=1}^n \\
 \bsc &= \biggl( \int_{\cx} C(\bsx,\bsx_i) \, (\nu -\hnu)(\D \bsx)\biggr)_{i=1}^n 
 =  \tbsc - \mC \bsw.
\end{align*}
Then the conditional probability density function of $\int_{\cx} f(\bsx) \,  (\nu -\hnu)(\D \bsx)$ given $\{f(\bsx_i )= y_i\}_{i=1}^n$ is $\rvarrho$, given by
\begin{equation*}
\rvarrho(y_0)  = \frac{\tvarrho(\tbsy)}{\varrho(\bsy)}  = \frac{\exp\bigl(-\frac 12 (\tbsy - m \tbsone)^T \tmC^{-1} (\tbsy - m \tbsone) + \frac 12 (\bsy - m \bsone)^T \mC^{-1} (\bsy - m \bsone)\bigr)}{\sqrt{2 \pi \det(\tmC)/\det(\mC)}}
\end{equation*}

To simplify the expression for $\rvarrho(y_0)$, we define
\[
\bsz = \mC^{-1} (\bsy - m \bsone), \qquad 
\begin{pmatrix} \tz_0 \\ \tbsz \end{pmatrix} = \tmC^{-1} (\tbsy - m \tbsone) 
= \begin{pmatrix} c_0 & \bsc^T \\ \bsc & \mC \end{pmatrix}^{-1} \begin{pmatrix} y_0 - m \\ \bsy - m \bsone \end{pmatrix}
\]
So,
\begin{gather*}
\begin{pmatrix} c_0 & \bsc^T \\ \bsc & \mC \end{pmatrix} \begin{pmatrix} \tz_0 \\ \tbsz \end{pmatrix} 
= \begin{pmatrix} y_0 - m(1 - \bsone^T\bsw) \\ \bsy - m \bsone \end{pmatrix} \\
\tbsz = \mC^{-1}[-\tz_0 \bsc + (\bsy - m \bsone )] \\
c_0 \tz_0 =y_0 - m(1 - \bsone^T\bsw)  - \bsc^T\tbsz =  y_0 - m(1 - \bsone^T\bsw)  - \bsc^T\mC^{-1}[-\tz_0 \bsc + (\bsy - m \bsone )] \\
(c_0 - \bsc^T\mC^{-1} \bsc) \tz_0 = y_0 - m(1 - \bsone^T\bsw)  - \bsc^T\mC^{-1}(\bsy - m \bsone )
\end{gather*}
\begin{align*}
(\tbsy - m \tbsone)^T \tmC^{-1} (\tbsy - m \tbsone) - (\bsy - m \bsone)^T \mC^{-1} (\bsy - 
m \bsone) \\
& = [y_0 - m(1 - \bsone^T\bsw)] \tz_0 + (\bsy - m \bsone)^T \tbsz - (\bsy - m \bsone)^T \mC^{-1} (\bsy - m \bsone) \\
& = \tz_0 [y_0 - m(1 - \bsone^T\bsw)  -  (\bsy - m \bsone)^T \mC^{-1} \bsc ] \\
& = \frac{[y_0 - m(1 - \bsone^T\bsw)  -  (\bsy - m \bsone)^T \mC^{-1} \bsc ]^2}{c_0 - \bsc^T\mC^{-1} \bsc}
\end{align*}
Using the formulas above we get
\begin{align*}
y_0 - m(1 - \bsone^T\bsw)  -  (\bsy - m \bsone)^T \mC^{-1} \bsc & = y_0 - m(1 - \bsone^T\bsw)  -  (\bsy - m \bsone)^T \mC^{-1}(\tbsc - \mC \bsw) \\
& = y_0 - m(1 - \mC^{-1}\tbsc) -  \bsy^T (\mC^{-1}\tbsc - \bsw) \\
c_0 - \bsc^T\mC^{-1} \bsc & = \tc_0 -2 \tbsc^T \bsw + \bsw^T \mC \bsw - (\tbsc - \mC \bsw)^T \mC^{-1} (\tbsc - \mC \bsw) \\
& =  \tc_0 - \tbsc ^T \mC^{-1} \tbsc
\end{align*}

So we may summarize as follows
\begin{align*}
\mu - \hmu \big \vert \{f(\bsx_i )= y_i\}_{i=1}^n &\sim \cn \bigl(m(1 - \mC^{-1}\tbsc) +  \bsy^T (\mC^{-1}\tbsc - \bsw), \tc_0 - \tbsc ^T \mC^{-1} \tbsc \bigr)
\end{align*}

\end{document}

Hi Fred,

Ill send a note to the steering committee and also the program committee.

Ive already asked some of them to come to mcqmc 2016, and Ill encourage them some more.  Nicolas Chopin is one of our plenary speakers and he is involved with that work.

I like your idea about messaging for cross-pollination, though Im not sure when the optimal time for it would be.

-Art



From: Fred Hickernell <hickernell@iit.edu>
Date: Friday, December 18, 2015 at 3:13 PM
To: mcqmc2016 <mcqmc2016@stanford.edu>
Cc: Pierre L'Ecuyer <lecuyer@iro.umontreal.ca>
Subject: Re: mcqmc 2016 tutorials

Hi Art,

I am actually on a family vacation sitting about 370 miles southeast of you in Arcadia, CA.  It is great to chat via email, and I hope that Pierre does not mind too much us including him on this.

Your thoughts below are quite helpful and provide a challenge to me to include in a gracious way the contributions from various corners.  Please both you and Pierre feel free to feed me ideas or articles that I should be aware of as I prepare my tutorial.  I want to be as fair as possible to everyone.

Yes, we need to encourage others from different perspectives and a variety of career levels to interact or be a part of MCQMC in a healthy way.  Erich Novaks recent revamping of the editorial board of the Journal of Complexity took aim at the old age problem, even if it may not of addressed the breadth of perspectives.  Yes, please share your ideas with the steering committee.  We did not have IBC or tractability at the beginning of MCQMC, so there is precedent for new ideas.  Should we ask someone to organize a probabilistic numerics special session?  As the chief organizer of MCQMC 2016, your opening remarks might mention your hope for more cross-pollination of ideas, your observation that the same thing may have different names in different contexts, and your encouragement to the audience to be kind in their questions. 

Best regards,
Fred


Fred J. Hickernell, Professor and Chair
Department of Applied Mathematics, Illinois Institute of Technology
RE Bldg Rm 208, 10 West 32nd Street, Chicago, IL 60616
hickernell@iit.edu, www.iit.edu/~hickernell
Office: 1 312 567 8983   Cell: 1 630 696 8124

On Dec 18, 2015, at 2:56 PM, mcqmc2016 <mcqmc2016@stanford.edu> wrote:

Dans email is below.  He was very prompt and helpful in his response. I saw his comment on Andrew Gelmans blog, right here http://andrewgelman.com/2015/12/07/28279/

Daniel.Simpson@math.ntnu.no

My thoughts on that group (and us) are as follows. Please do not share them.  They are young and energetic and quite smart. I think they dont know so much about the work that has gone before, and these factors will combine to get them to reinvent or rename many things. But I also expect they will eventually push our field forward in a positive way and bring new problem areas to our attention. At Banff I saw some young speakers get fairly harsh questioning. I hope that doesnt happen to the probabilistic numerics people. The risk is that they will not feel welcome, and then theyll just ignore/go around our field.  The MCQMC crowd is skewed towards older people compared to computer science and even (I think) compared to statistics.  I think it is healthiest to have people from the full range of career levels, and Im very happy to see them taking an interest in the problems we study, but from a different point of view.  [If you think I ought to share these ideas with somebody else, such as the steering committee, let me know, and Ill draft a version for them.]

-Art


From: Fred Hickernell <hickernell@iit.edu>
Date: Friday, December 18, 2015 at 2:37 PM
To: mcqmc2016 <mcqmc2016@stanford.edu>
Cc: Pierre L'Ecuyer <lecuyer@iro.umontreal.ca>
Subject: Re: mcqmc 2016 tutorials

Thanks Art,

Your remarks, and that of Dans are something that I am aware of through connections with Greg Fasshauer and his friends, but I will need to refresh my memory of them.  So this closes the circle a bit.  Can you give me Dan Simpsons contact info in case I would like to contact him?  Any thoughts on the website http://probabilistic-numerics.org or the people behind it?

Best regards,
Fred


Fred J. Hickernell, Professor and Chair
Department of Applied Mathematics, Illinois Institute of Technology
RE Bldg Rm 208, 10 West 32nd Street, Chicago, IL 60616
hickernell@iit.edu, www.iit.edu/~hickernell
Office: 1 312 567 8983   Cell: 1 630 696 8124

On Dec 18, 2015, at 1:59 PM, mcqmc2016 <mcqmc2016@stanford.edu> wrote:

Hi Fred,

I remember Persis article.  He showed that you can get the midpoint rule from some Gaussian prior and I think other priors would lead to Simpsons rule.  If I recall correctly, Poincare may have said some related things.

Youre right that they assume a lot of smoothness.  Some of the methods require O(n^3) computation so you have to attain a very good rate of convergence to beat an O(1) computation method.  There are also numerical limits.  Exponentially fast convergence happens with exponentially worsening condition number and that loses accuracy.  The description below is copy/pasted from an email I got from Dan Simpson.

Ive encouraged the probabilistic numerics people to come to mcqmc.  I think that they have something to offer to the uncertainty quantification people. Their functions may take 12 hours to run and so n^3 for them is still cheaper than one function evaluation.

-Art

============================================================

Hi Art,

I actually mis-remembered the result (it's still bad, but there are smoothness assumptions!).  I learnt about all of this from the Radial Basis Function literature, that usually uses infinitely smooth functions, so exponentially large condition numbers occur.  It's only polynomial (with a power depending on dimension) if you use a Matrn Kernel.  So, by the usual rule-of-thumb, a Matern with smoothness nu will have lose up to O(d*log_{10}(1/h)) digits accuracy, while an squared-exponential covariance function will lose up to O(d^2/h^2) digits.  (Here d is the dimension and h is the minimum distance between two points in the interpolation set.)

The classic paper on this is by Robert Schaback (who gave the world compactly supported RBFs!), and I've got the bibtex ref below.  He basically proves an uncertainty principle that says that the higher the accuracy, the worse the conditioning.  The argument (in stats language) hinges on the idea that the covariance matrix approximates in a well understood way the covariance operator for the GP, which has known spectrum.  Then it comes down to how quickly a point-set resolves the high-frequency features.   It's a really nice piece of work.

Link that may not work:  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.8132&rep=rep1&type=pdf


The story doesn't end there.  There's some more recent work that suggests that things aren't completely terrible, you just need to use a different basis for the finite dimensional vector space.  In particular, you can get that the Lebesgue constant (the sup-norm condition number of the interpolation process) grows like N^{1/2} as for Matrn-like kernels if the function is interpolated at quasi-uniform points.  It's also possible to build an orthogonal basis for the interpolation space, which would probably decrease the condition number.

Theory: http://num.math.uni-goettingen.de/schaback/research/papers/SoKBI.pdf
Construction: http://www.math.unipd.it/~demarchi/papers/FCoOB.pdf


Hope this helps,

Dan




@article{schaback1995error,
  title={Error estimates and condition numbers for radial basis function interpolation},
  author={Schaback, Robert},
  journal={Advances in Computational Mathematics},
  volume={3},
  number={3},
  pages={251--264},
  year={1995},
  publisher={Springer}
}


From: Fred Hickernell <hickernell@iit.edu>
Date: Friday, December 18, 2015 at 1:06 PM
To: mcqmc2016 <mcqmc2016@stanford.edu>
Cc: Art Owen <owen@stanford.edu>, Pierre L'Ecuyer <lecuyer@iro.umontreal.ca>
Subject: Re: mcqmc 2016 tutorials

Dear Art,

Thanks for pointing out this work.  I need to be aware of the ideas in the community outside traditional MCQMC so that I can mention them and put them into context with the traditional MCQMC work that I am more familiar with.

According to my understanding Persi Diaconis introduced the idea of probabilistic numerics many years back, but that it was actually around earlier via the IBC folks.  I was not aware of the Frank-Wolfe ideas that were mentioned in the paper that Christian Robert talked about.  My guess is that the wonderful convergence rates that they are talking about assume derivatives of all orders.  I need to study this paper.  Anything else from the NIPS 2015 conference that I should look at?

Best regards,
Fred


Fred J. Hickernell, Professor and Chair
Department of Applied Mathematics, Illinois Institute of Technology
RE Bldg Rm 208, 10 West 32nd Street, Chicago, IL 60616
hickernell@iit.edu, www.iit.edu/~hickernell
Office: 1 312 567 8983   Cell: 1 630 696 8124

On Dec 17, 2015, at 6:48 PM, mcqmc2016 <mcqmc2016@stanford.edu> wrote:

Hi Fred,

There was a `probabilistic numerics session at NIPS in Montreal.
Those are the people starting to get into RKHS from a machine
Learning background.  They also call It Bayesian numerical analysis.
Christian Robert has been blogging about it recently too.

It seems to me that they are kriging in an RKHS context.  I think they will migrate to sensitivity analysis. At least I nudged them in that direction.

Im also going to ask Andrew Gelman to come to MCQMC and talk about Stan. I hope that some of his people will want to learn about QMC.

Mainly I think that the North American researchers skew heavily to MCMC and other MC things that are not QMC.

-Art


From: Fred Hickernell <hickernell@iit.edu>
Date: Thursday, December 17, 2015 at 5:27 PM
To: Art Owen <owen@stanford.edu>
Cc: Pierre L'Ecuyer <lecuyer@iro.umontreal.ca>, mcqmc2016 <mcqmc2016@stanford.edu>
Subject: Re: mcqmc 2016 tutorials

Dear Art (and Pierre),

Yes, I am willing to give a tutorial, and I like the idea of our two tutorials being parts of a whole.  We can exchange notes as we plan our talks.  

Art, please enlighten me about what would appeal to MCMC and machine learning folks about RKHS.  In my mind RKHS is familiar to a lot of machine learning folk, e.g., support vector machines, but perhaps I am missing something.







Best regards,
Fred


Fred J. Hickernell, Professor and Chair
Department of Applied Mathematics, Illinois Institute of Technology
RE Bldg Rm 208, 10 West 32nd Street, Chicago, IL 60616
hickernell@iit.edu, www.iit.edu/~hickernell
Office: 1 312 567 8983   Cell: 1 630 696 8124

On Dec 16, 2015, at 1:51 PM, Art B Owen <owen@stanford.edu> wrote:


I think it makes sense to treat it as a 
two part tutorial.  I agree that one hour
is short for the material.  But multiple
hours can be hard for the speaker and
audience.

The idea is to reach people from MCMC
or machine learning or other areas who
are new to QMC.  Some of them want
Hilbert spaces and some don't.

Adding a bit of cutting edge material would
also be welcome because you will probably
have some QMC-regulars listening in to pick
up insights.

-Art


From: Pierre Lecuyer <lecuyer@iro.umontreal.ca>
Sent: Wednesday, December 16, 2015 8:05 AM
To: Fred Hickernell; mcqmc2016
Subject: Re: mcqmc 2016 tutorials
 
Fred:

Thanks for your thoughts.  I think we can synchronize between ourselves when time comes and exchange our slides.  I can avoid discussing and even mentioning RKHS. Multilevel combined with RQMC: I may give one concrete example of that (showing that applying multilevel can sometimes makes the function much less RQMC-friendly), but no theory on multilevel.  Ok?

-- Pierre

On 16/12/2015 10:47 AM, Fred Hickernell wrote:
Dear Art,

Thank you for the kind invitation.  It sounds like a great opportunity and challenge.

It would good to synchronize our thoughts.  Although a tutorial as you describe need not focus only on recent work, my recent attention has been on the Fourier exponential/Walsh series representations of the integrand that I use to derive data-based cubature error bounds.  Would that be a part of Pierres tutorial or could it be a part of mine?  There is a connection between RKHS and the series representations if you decompose the kernel or look at (digital) shift invariant kernels.  Would there be mention of multi-level methods and their errors?

Best regards,
Fred


Fred J. Hickernell, Professor and Chair
Department of Applied Mathematics, Illinois Institute of Technology
RE Bldg Rm 208, 10 West 32nd Street, Chicago, IL 60616
hickernell@iit.edu, www.iit.edu/~hickernell
Office: 1 312 567 8983   Cell: 1 630 696 8124

On Dec 14, 2015, at 6:13 PM, MCQMC 2016 <mcqmc2016@stanford.edu> wrote:

What I have in mind is one hour
on QMC and discrepancy and RQMC
showing nets and lattices, but aimed
at people from machine learning and
the MCMC world and maybe the particle
people too.

Then the second hour would be on
reproducing kernel Hilbert spaces,
Sobolev spaces, tractability and so
on.  Again aimed at people who are
new to QMC.

-Art

... I would be interested to know what
the WSC highlights were.


On 12/14/15 4:13 PM, Pierre Lecuyer wrote:
Art:

Thank you for this kind invitation.  If you think I am good enough to give a tutorial on QMC + RQMC, I can do that.  This would give me pressure to get up to date with all the latest stuff!

What level do you have in mind?   How long? Parallel tutorials like we had in Montreal, or shorter ones in series?

P.S.  Just back from L.A. hours ago; was the WSC Conference.  Then Joshua Tree Nat Park.   Fabulous!

-- Pierre


On 14/12/2015 3:33 PM, MCQMC 2016 wrote:
Dear Fred and Pierre,

Are you interested in giving a tutorial
at mcqmc 2016?

What I have in mind is a tutorial from
Pierre on QMC in general followed by a
tutorial from Fred on RKHS.  Then maybe
a third tutorial on a different topic.  With
short refreshment breaks in between.

The format is 1 hour and the time is
Sunday mid-afternoon.

I can waive registration charges and if
expenses are a problem, I can help with
those too.

Best regards,

-Art









-- 
Pierre L'Ecuyer, Professeur Titulaire
Chaire du Canada en Simulation et Optimisation Stochastique
CIRRELT, GERAD, and DIRO, Universit de Montral, Canada
http://www.iro.umontreal.ca/~lecuyer

